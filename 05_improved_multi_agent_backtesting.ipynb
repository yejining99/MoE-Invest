{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ê°œì„ ëœ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ë°±í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ (ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í¬í•¨)\n",
        "\n",
        "5ê°œ íˆ¬ì ì—ì´ì „íŠ¸ì˜ ì„±ê³¼ë¥¼ ë°±í…ŒìŠ¤íŒ…í•˜ë˜, ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ(ì¤‘ë³µ í‹°ì»¤, ì˜¤íƒ€ ë“±)ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ìˆ˜ì •í•©ë‹ˆë‹¤.\n",
        "\n",
        "## ğŸ”§ ê°œì„  ì‚¬í•­\n",
        "- âœ… **ì¤‘ë³µ í‹°ì»¤ ê°ì§€ ë° ê°€ì¤‘ì¹˜ í•©ì‚°**\n",
        "- âœ… **í‹°ì»¤ ì˜¤íƒ€ ìë™ ìˆ˜ì •** (ì˜ˆ: CPTR â†’ CPRT)\n",
        "- âœ… **ìœ íš¨í•˜ì§€ ì•Šì€ í‹°ì»¤ ì œê±°**\n",
        "- âœ… **ê°€ì¤‘ì¹˜ ì •ê·œí™”** (í•©ê³„ê°€ 100%ê°€ ë˜ë„ë¡)\n",
        "- âœ… **ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ì œê³µ**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê°œì„ ëœ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ë°±í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ (1/3) - ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í´ë˜ìŠ¤\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "from collections import Counter\n",
        "import difflib\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import yfinance as yf\n",
        "\n",
        "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "class ImprovedMultiAgentBacktester:\n",
        "    \"\"\"ë°ì´í„° í’ˆì§ˆ ê²€ì¦ì´ í¬í•¨ëœ 5ê°œ íˆ¬ì ì—ì´ì „íŠ¸ í†µí•© ë°±í…ŒìŠ¤íŒ… í´ë˜ìŠ¤\"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 results_dir=\"results\",\n",
        "                 ohlcv_path=\"data/nasdaq100_ohlcv.csv\",\n",
        "                 benchmark_cache_path=\"data/benchmark_data.csv\",\n",
        "                 transaction_cost=0.0001):\n",
        "        \n",
        "        self.results_dir = results_dir\n",
        "        self.ohlcv_path = ohlcv_path\n",
        "        self.benchmark_cache_path = benchmark_cache_path\n",
        "        self.transaction_cost = transaction_cost\n",
        "        \n",
        "        # ì—ì´ì „íŠ¸ ì •ë³´ ì •ì˜\n",
        "        self.agents = {\n",
        "            'graham': {\n",
        "                'name': 'Benjamin Graham',\n",
        "                'directory': 'graham_agent',\n",
        "                'portfolio_prefix': 'graham_portfolio_',\n",
        "                'color': 'blue'\n",
        "            },\n",
        "            'buffett': {\n",
        "                'name': 'Warren Buffett',\n",
        "                'directory': 'buffett_agent',\n",
        "                'portfolio_prefix': 'buffett_portfolio_',\n",
        "                'color': 'red'\n",
        "            },\n",
        "            'greenblatt': {\n",
        "                'name': 'Joel Greenblatt',\n",
        "                'directory': 'greenblatt_agent',\n",
        "                'portfolio_prefix': 'greenblatt_portfolio_',\n",
        "                'color': 'green'\n",
        "            },\n",
        "            'piotroski': {\n",
        "                'name': 'Joseph Piotroski',\n",
        "                'directory': 'piotroski_agent',\n",
        "                'portfolio_prefix': 'piotroski_portfolio_',\n",
        "                'color': 'orange'\n",
        "            },\n",
        "            'altman': {\n",
        "                'name': 'Edward Altman',\n",
        "                'directory': 'altman_agent',\n",
        "                'portfolio_prefix': 'altman_portfolio_',\n",
        "                'color': 'purple'\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # ë²¤ì¹˜ë§ˆí¬ í‹°ì»¤ë“¤ ì •ì˜\n",
        "        self.benchmarks = {\n",
        "            'QQQ': {'name': 'NASDAQ 100 ETF', 'color': 'black'},\n",
        "            'SPY': {'name': 'S&P 500 ETF', 'color': 'gray'}\n",
        "        }\n",
        "        \n",
        "        # ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ ì €ì¥ìš©\n",
        "        self.backtest_results = {}\n",
        "        \n",
        "        # ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ ë¡œê·¸\n",
        "        self.data_quality_issues = []\n",
        "        \n",
        "    def validate_and_clean_ticker(self, ticker, valid_tickers=None):\n",
        "        \"\"\"í‹°ì»¤ ê²€ì¦ ë° ì •ì •\"\"\"\n",
        "        if valid_tickers is None:\n",
        "            # NASDAQ 100 í‹°ì»¤ ëª©ë¡ì—ì„œ ìœ íš¨í•œ í‹°ì»¤ í™•ì¸\n",
        "            valid_tickers = set(self.ohlcv_df['TICKERSYMBOL'].unique())\n",
        "        \n",
        "        ticker = ticker.strip().upper()\n",
        "        \n",
        "        # ì •í™•í•œ ë§¤ì¹˜ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
        "        if ticker in valid_tickers:\n",
        "            return ticker\n",
        "        \n",
        "        # ìœ ì‚¬í•œ í‹°ì»¤ ì°¾ê¸° (ì˜¤íƒ€ ìˆ˜ì •)\n",
        "        close_matches = difflib.get_close_matches(ticker, valid_tickers, n=1, cutoff=0.8)\n",
        "        if close_matches:\n",
        "            corrected_ticker = close_matches[0]\n",
        "            self.data_quality_issues.append({\n",
        "                'type': 'ticker_correction',\n",
        "                'original': ticker,\n",
        "                'corrected': corrected_ticker,\n",
        "                'message': f\"í‹°ì»¤ '{ticker}'ë¥¼ '{corrected_ticker}'ë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.\"\n",
        "            })\n",
        "            return corrected_ticker\n",
        "        \n",
        "        # ë§¤ì¹˜ë˜ëŠ” ê²ƒì´ ì—†ìœ¼ë©´ None ë°˜í™˜\n",
        "        self.data_quality_issues.append({\n",
        "            'type': 'invalid_ticker',\n",
        "            'original': ticker,\n",
        "            'message': f\"ìœ íš¨í•˜ì§€ ì•Šì€ í‹°ì»¤ '{ticker}'ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
        "        })\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„° ì •ë¦¬ ë° ê²€ì¦ ë©”ì„œë“œë“¤ (2/3)\n",
        "\n",
        "def clean_portfolio_data(self, df, agent_key):\n",
        "    \"\"\"í¬íŠ¸í´ë¦¬ì˜¤ ë°ì´í„° ì •ë¦¬ ë° ê²€ì¦\"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        return df\n",
        "        \n",
        "    # ì›ë³¸ ë°ì´í„° ë°±ì—…\n",
        "    original_df = df.copy()\n",
        "    \n",
        "    # 1. ë¹ˆ í–‰ ì œê±°\n",
        "    df = df.dropna(subset=['Ticker'])\n",
        "    \n",
        "    # 2. í‹°ì»¤ ê²€ì¦ ë° ì •ì •\n",
        "    valid_tickers = set(self.ohlcv_df['TICKERSYMBOL'].unique())\n",
        "    df['Original_Ticker'] = df['Ticker'].copy()\n",
        "    df['Ticker'] = df['Ticker'].apply(lambda x: self.validate_and_clean_ticker(x, valid_tickers))\n",
        "    \n",
        "    # 3. ìœ íš¨í•˜ì§€ ì•Šì€ í‹°ì»¤ ì œê±°\n",
        "    invalid_mask = df['Ticker'].isnull()\n",
        "    if invalid_mask.any():\n",
        "        invalid_tickers = df[invalid_mask]['Original_Ticker'].tolist()\n",
        "        self.data_quality_issues.append({\n",
        "            'type': 'removed_invalid_tickers',\n",
        "            'agent': self.agents[agent_key]['name'],\n",
        "            'tickers': invalid_tickers,\n",
        "            'message': f\"{self.agents[agent_key]['name']}ì—ì„œ ìœ íš¨í•˜ì§€ ì•Šì€ í‹°ì»¤ {len(invalid_tickers)}ê°œë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤: {invalid_tickers}\"\n",
        "        })\n",
        "        df = df[~invalid_mask]\n",
        "    \n",
        "    # 4. ì¤‘ë³µ í‹°ì»¤ ì²˜ë¦¬\n",
        "    duplicate_tickers = df[df.duplicated('Ticker', keep=False)]['Ticker'].tolist()\n",
        "    if duplicate_tickers:\n",
        "        # ì¤‘ë³µëœ í‹°ì»¤ë“¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ í•©ì‚°\n",
        "        self.data_quality_issues.append({\n",
        "            'type': 'duplicate_tickers',\n",
        "            'agent': self.agents[agent_key]['name'],\n",
        "            'tickers': list(set(duplicate_tickers)),\n",
        "            'message': f\"{self.agents[agent_key]['name']}ì—ì„œ ì¤‘ë³µ í‹°ì»¤ {len(set(duplicate_tickers))}ê°œë¥¼ ë°œê²¬í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ í•©ì‚°í–ˆìŠµë‹ˆë‹¤: {list(set(duplicate_tickers))}\"\n",
        "        })\n",
        "        \n",
        "        # ì¤‘ë³µ í‹°ì»¤ì˜ ê°€ì¤‘ì¹˜ í•©ì‚°\n",
        "        df = df.groupby('Ticker').agg({\n",
        "            'Weight (%)': 'sum',\n",
        "            'Score': 'mean',  # ScoreëŠ” í‰ê· \n",
        "            'Reason': 'first'  # Reasonì€ ì²« ë²ˆì§¸ ê²ƒ ì‚¬ìš©\n",
        "        }).reset_index()\n",
        "    \n",
        "    # 5. ê°€ì¤‘ì¹˜ ì •ê·œí™”\n",
        "    total_weight = df['Weight (%)'].sum()\n",
        "    if abs(total_weight - 100.0) > 0.01:  # 0.01% ì´ìƒ ì°¨ì´ë‚˜ë©´ ì •ê·œí™”\n",
        "        self.data_quality_issues.append({\n",
        "            'type': 'weight_normalization',\n",
        "            'agent': self.agents[agent_key]['name'],\n",
        "            'original_total': total_weight,\n",
        "            'message': f\"{self.agents[agent_key]['name']}ì˜ ì´ ê°€ì¤‘ì¹˜ê°€ {total_weight:.2f}%ì˜€ìœ¼ë¯€ë¡œ 100%ë¡œ ì •ê·œí™”í–ˆìŠµë‹ˆë‹¤.\"\n",
        "        })\n",
        "        df['Weight (%)'] = df['Weight (%)'] / total_weight * 100.0\n",
        "    \n",
        "    # 6. ê°€ì¤‘ì¹˜ê°€ 0 ì´í•˜ì¸ í•­ëª© ì œê±°\n",
        "    zero_weight_mask = df['Weight (%)'] <= 0\n",
        "    if zero_weight_mask.any():\n",
        "        zero_weight_tickers = df[zero_weight_mask]['Ticker'].tolist()\n",
        "        self.data_quality_issues.append({\n",
        "            'type': 'removed_zero_weight',\n",
        "            'agent': self.agents[agent_key]['name'],\n",
        "            'tickers': zero_weight_tickers,\n",
        "            'message': f\"{self.agents[agent_key]['name']}ì—ì„œ ê°€ì¤‘ì¹˜ê°€ 0 ì´í•˜ì¸ í‹°ì»¤ {len(zero_weight_tickers)}ê°œë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤: {zero_weight_tickers}\"\n",
        "        })\n",
        "        df = df[~zero_weight_mask]\n",
        "    \n",
        "    # Original_Ticker ì»¬ëŸ¼ ì œê±°\n",
        "    if 'Original_Ticker' in df.columns:\n",
        "        df = df.drop('Original_Ticker', axis=1)\n",
        "    \n",
        "    return df\n",
        "    \n",
        "def print_data_quality_report(self):\n",
        "    \"\"\"ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ ë¦¬í¬íŠ¸ ì¶œë ¥\"\"\"\n",
        "    if not self.data_quality_issues:\n",
        "        print(\"âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì™„ë£Œ: ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "        \n",
        "    print(\"\\\\n\" + \"=\" * 80)\n",
        "    print(\"ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ ë¦¬í¬íŠ¸\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # ë¬¸ì œ ìœ í˜•ë³„ ë¶„ë¥˜\n",
        "    issue_types = Counter([issue['type'] for issue in self.data_quality_issues])\n",
        "    \n",
        "    print(f\"\\\\nì´ {len(self.data_quality_issues)}ê°œì˜ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œë¥¼ ë°œê²¬í•˜ê³  ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤:\")\n",
        "    for issue_type, count in issue_types.items():\n",
        "        type_names = {\n",
        "            'ticker_correction': 'í‹°ì»¤ ì˜¤íƒ€ ìˆ˜ì •',\n",
        "            'invalid_ticker': 'ìœ íš¨í•˜ì§€ ì•Šì€ í‹°ì»¤',\n",
        "            'duplicate_tickers': 'ì¤‘ë³µ í‹°ì»¤',\n",
        "            'weight_normalization': 'ê°€ì¤‘ì¹˜ ì •ê·œí™”',\n",
        "            'removed_invalid_tickers': 'ìœ íš¨í•˜ì§€ ì•Šì€ í‹°ì»¤ ì œê±°',\n",
        "            'removed_zero_weight': '0 ê°€ì¤‘ì¹˜ í‹°ì»¤ ì œê±°'\n",
        "        }\n",
        "        print(f\"  - {type_names.get(issue_type, issue_type)}: {count}ê±´\")\n",
        "    \n",
        "    print(\"\\\\n[ìƒì„¸ ë‚´ìš©]\")\n",
        "    for i, issue in enumerate(self.data_quality_issues, 1):\n",
        "        print(f\"{i:2d}. {issue['message']}\")\n",
        "    \n",
        "    print(\"\\\\n\" + \"=\" * 80)\n",
        "\n",
        "# í´ë˜ìŠ¤ì— ë©”ì„œë“œ ì¶”ê°€\n",
        "ImprovedMultiAgentBacktester.clean_portfolio_data = clean_portfolio_data\n",
        "ImprovedMultiAgentBacktester.print_data_quality_report = print_data_quality_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê¸°ì¡´ MultiAgentBacktesterì˜ ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ì„ ë³µì‚¬í•´ì„œ ì‚¬ìš©\n",
        "# (ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°, ë°±í…ŒìŠ¤íŒ… ê³„ì‚°, ì‹œê°í™” ë“±)\n",
        "\n",
        "# ê¸°ì¡´ 04_multi_agent_backtesting.ipynbì˜ ë©”ì„œë“œë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜ \n",
        "# ë°ì´í„° ë¡œë”© ë¶€ë¶„ì— í’ˆì§ˆ ê²€ì¦ì„ ì¶”ê°€\n",
        "\n",
        "def load_data(self):\n",
        "    \"\"\"í•„ìš”í•œ ë°ì´í„°ë“¤ì„ ë¡œë“œ (í’ˆì§ˆ ê²€ì¦ í¬í•¨)\"\"\"\n",
        "    print(\"ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
        "    \n",
        "    # OHLCV ë°ì´í„° ë¡œë“œ\n",
        "    print(\"  - OHLCV ë°ì´í„° ë¡œë”©...\")\n",
        "    self.ohlcv_df = pd.read_csv(self.ohlcv_path)\n",
        "    self.ohlcv_df['EVAL_D'] = pd.to_datetime(self.ohlcv_df['EVAL_D'])\n",
        "    \n",
        "    # ê° ì—ì´ì „íŠ¸ë³„ í¬íŠ¸í´ë¦¬ì˜¤ íŒŒì¼ë“¤ ë¡œë“œ\n",
        "    print(\"  - ì—ì´ì „íŠ¸ë³„ í¬íŠ¸í´ë¦¬ì˜¤ ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë”©...\")\n",
        "    self.agent_portfolios = {}\n",
        "    for agent_key, agent_info in self.agents.items():\n",
        "        self.load_agent_portfolio_files(agent_key, agent_info)\n",
        "    \n",
        "    # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë“œ\n",
        "    print(\"  - ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë”©...\")\n",
        "    self.benchmark_df = self.load_benchmark_data()\n",
        "    \n",
        "    print(\"ë°ì´í„° ë¡œë”© ì™„ë£Œ!\")\n",
        "    \n",
        "    # ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ì¶œë ¥\n",
        "    self.print_data_quality_report()\n",
        "    \n",
        "def load_agent_portfolio_files(self, agent_key, agent_info):\n",
        "    \"\"\"íŠ¹ì • ì—ì´ì „íŠ¸ì˜ í¬íŠ¸í´ë¦¬ì˜¤ CSV íŒŒì¼ë“¤ì„ ë¡œë“œ (ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í¬í•¨)\"\"\"\n",
        "    portfolio_dir = os.path.join(self.results_dir, agent_info['directory'])\n",
        "    portfolio_files = glob.glob(f\"{portfolio_dir}/{agent_info['portfolio_prefix']}*.csv\")\n",
        "    \n",
        "    portfolios = []\n",
        "    \n",
        "    for file in sorted(portfolio_files):\n",
        "        try:\n",
        "            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ\n",
        "            filename = os.path.basename(file)\n",
        "            date_part = filename.replace(agent_info['portfolio_prefix'], '').replace('.csv', '')\n",
        "            start_date, end_date = date_part.split('_')\n",
        "            \n",
        "            # CSV íŒŒì¼ ë¡œë“œ\n",
        "            df = pd.read_csv(file)\n",
        "            if len(df) == 0 or (len(df) == 1 and df.iloc[0].isnull().all()):\n",
        "                continue\n",
        "            \n",
        "            # ğŸ”§ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ì •ë¦¬ (ì´ ë¶€ë¶„ì´ í•µì‹¬ ê°œì„ ì‚¬í•­!)\n",
        "            df = self.clean_portfolio_data(df, agent_key)\n",
        "            \n",
        "            if len(df) > 0:\n",
        "                df['analysis_start'] = pd.to_datetime(start_date)\n",
        "                df['analysis_end'] = pd.to_datetime(end_date)\n",
        "                df['investment_start'] = df['analysis_end'] + timedelta(days=1)\n",
        "                next_quarter_start = df['analysis_end'] + timedelta(days=90)\n",
        "                df['investment_end'] = next_quarter_start\n",
        "                \n",
        "                portfolios.append(df)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {file}, ì˜¤ë¥˜: {e}\")\n",
        "            \n",
        "    if portfolios:\n",
        "        self.agent_portfolios[agent_key] = pd.concat(portfolios, ignore_index=True)\n",
        "        print(f\"  - {agent_info['name']}: {len(portfolios)}ê°œ ë¶„ê¸° í¬íŠ¸í´ë¦¬ì˜¤ ë¡œë“œë¨\")\n",
        "    else:\n",
        "        print(f\"  - {agent_info['name']}: í¬íŠ¸í´ë¦¬ì˜¤ ë°ì´í„° ì—†ìŒ\")\n",
        "        self.agent_portfolios[agent_key] = None\n",
        "\n",
        "# í´ë˜ìŠ¤ì— ë©”ì„œë“œ ì¶”ê°€\n",
        "ImprovedMultiAgentBacktester.load_data = load_data\n",
        "ImprovedMultiAgentBacktester.load_agent_portfolio_files = load_agent_portfolio_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê°œì„ ëœ ë°±í…ŒìŠ¤í„° ì´ˆê¸°í™” ë° ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹¤í–‰\n",
        "print(\"ğŸ”§ ê°œì„ ëœ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ë°±í…ŒìŠ¤í„° ì´ˆê¸°í™” ì¤‘...\")\n",
        "backtester = ImprovedMultiAgentBacktester(transaction_cost=0.0001)\n",
        "\n",
        "# ë°ì´í„° ë¡œë”© (ìë™ìœ¼ë¡œ í’ˆì§ˆ ê²€ì¦ ìˆ˜í–‰)\n",
        "backtester.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Edward Altman ì—ì´ì „íŠ¸ì˜ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ ì§ì ‘ í™•ì¸\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def check_altman_data_quality():\n",
        "    \"\"\"Altman ì—ì´ì „íŠ¸ì˜ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œë¥¼ í™•ì¸\"\"\"\n",
        "    print(\"ğŸ” Edward Altman ì—ì´ì „íŠ¸ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    portfolio_dir = \"results/altman_agent\"\n",
        "    portfolio_files = glob.glob(f\"{portfolio_dir}/altman_portfolio_*.csv\")\n",
        "    \n",
        "    issues_found = []\n",
        "    \n",
        "    for file in sorted(portfolio_files):\n",
        "        filename = os.path.basename(file)\n",
        "        print(f\"\\\\nğŸ“ {filename}\")\n",
        "        \n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            if len(df) == 0:\n",
        "                continue\n",
        "                \n",
        "            # 1. ì¤‘ë³µ í‹°ì»¤ ê²€ì‚¬\n",
        "            duplicates = df[df.duplicated('Ticker', keep=False)]\n",
        "            if len(duplicates) > 0:\n",
        "                duplicate_tickers = duplicates['Ticker'].unique().tolist()\n",
        "                print(f\"  âš ï¸  ì¤‘ë³µ í‹°ì»¤ ë°œê²¬: {duplicate_tickers}\")\n",
        "                issues_found.append(f\"{filename}: ì¤‘ë³µ í‹°ì»¤ {duplicate_tickers}\")\n",
        "                \n",
        "                # ì¤‘ë³µ ìƒì„¸ ì •ë³´ ì¶œë ¥\n",
        "                for ticker in duplicate_tickers:\n",
        "                    ticker_rows = df[df['Ticker'] == ticker]\n",
        "                    print(f\"    - {ticker}: {len(ticker_rows)}ë²ˆ ë“±ì¥, ê°€ì¤‘ì¹˜ í•©ê³„: {ticker_rows['Weight (%)'].sum():.2f}%\")\n",
        "            \n",
        "            # 2. ê°€ì¤‘ì¹˜ í•©ê³„ ê²€ì‚¬\n",
        "            total_weight = df['Weight (%)'].sum()\n",
        "            if abs(total_weight - 100.0) > 0.01:\n",
        "                print(f\"  âš ï¸  ê°€ì¤‘ì¹˜ í•©ê³„ ì´ìƒ: {total_weight:.2f}% (100%ê°€ ì•„ë‹˜)\")\n",
        "                issues_found.append(f\"{filename}: ê°€ì¤‘ì¹˜ í•©ê³„ {total_weight:.2f}%\")\n",
        "            \n",
        "            # 3. ì´ìƒí•œ í‹°ì»¤ ê²€ì‚¬ (ì˜ˆ: CPTR)\n",
        "            suspicious_tickers = []\n",
        "            for ticker in df['Ticker'].unique():\n",
        "                if ticker in ['CPTR']:  # CPRTì˜ ì˜¤íƒ€ë¡œ ì¶”ì •\n",
        "                    suspicious_tickers.append(ticker)\n",
        "            \n",
        "            if suspicious_tickers:\n",
        "                print(f\"  âš ï¸  ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‹°ì»¤: {suspicious_tickers} (ì˜¤íƒ€ ê°€ëŠ¥ì„±)\")\n",
        "                issues_found.append(f\"{filename}: ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í‹°ì»¤ {suspicious_tickers}\")\n",
        "            \n",
        "            if len(duplicates) == 0 and abs(total_weight - 100.0) <= 0.01 and len(suspicious_tickers) == 0:\n",
        "                print(\"  âœ… ë¬¸ì œ ì—†ìŒ\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
        "            issues_found.append(f\"{filename}: íŒŒì¼ ì½ê¸° ì˜¤ë¥˜\")\n",
        "    \n",
        "    print(f\"\\\\nğŸ“‹ ì´ {len(issues_found)}ê°œì˜ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ ë°œê²¬:\")\n",
        "    for issue in issues_found:\n",
        "        print(f\"  - {issue}\")\n",
        "    \n",
        "    return issues_found\n",
        "\n",
        "# ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹¤í–‰\n",
        "issues = check_altman_data_quality()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹¤í–‰\n",
        "issues = check_altman_data_quality()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ ìë™ ìˆ˜ì • í•¨ìˆ˜\n",
        "import shutil\n",
        "\n",
        "def fix_altman_data_quality():\n",
        "    \"\"\"Altman ì—ì´ì „íŠ¸ì˜ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì •\"\"\"\n",
        "    print(\"ğŸ”§ Edward Altman ì—ì´ì „íŠ¸ ë°ì´í„° í’ˆì§ˆ ìë™ ìˆ˜ì •\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    portfolio_dir = \"results/altman_agent\"\n",
        "    portfolio_files = glob.glob(f\"{portfolio_dir}/altman_portfolio_*.csv\")\n",
        "    \n",
        "    fixes_applied = []\n",
        "    \n",
        "    for file in sorted(portfolio_files):\n",
        "        filename = os.path.basename(file)\n",
        "        print(f\"\\\\nğŸ“ {filename} ì²˜ë¦¬ ì¤‘...\")\n",
        "        \n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            if len(df) == 0:\n",
        "                continue\n",
        "                \n",
        "            original_df = df.copy()\n",
        "            fixes_for_file = []\n",
        "            \n",
        "            # 1. í‹°ì»¤ ì˜¤íƒ€ ìˆ˜ì • (CPTR â†’ CPRT)\n",
        "            if 'CPTR' in df['Ticker'].values:\n",
        "                df.loc[df['Ticker'] == 'CPTR', 'Ticker'] = 'CPRT'\n",
        "                fixes_for_file.append(\"CPTR â†’ CPRT ì˜¤íƒ€ ìˆ˜ì •\")\n",
        "                print(\"  âœ… CPTRë¥¼ CPRTë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.\")\n",
        "            \n",
        "            # 2. ì¤‘ë³µ í‹°ì»¤ ì²˜ë¦¬ (ê°€ì¤‘ì¹˜ í•©ì‚°)\n",
        "            duplicates = df[df.duplicated('Ticker', keep=False)]\n",
        "            if len(duplicates) > 0:\n",
        "                duplicate_tickers = duplicates['Ticker'].unique().tolist()\n",
        "                print(f\"  ğŸ”„ ì¤‘ë³µ í‹°ì»¤ ì²˜ë¦¬: {duplicate_tickers}\")\n",
        "                \n",
        "                # ì¤‘ë³µ í‹°ì»¤ì˜ ê°€ì¤‘ì¹˜ í•©ì‚°\n",
        "                df_fixed = df.groupby('Ticker').agg({\n",
        "                    'Weight (%)': 'sum',\n",
        "                    'Score': 'mean',\n",
        "                    'Reason': 'first'\n",
        "                }).reset_index()\n",
        "                \n",
        "                df = df_fixed\n",
        "                fixes_for_file.append(f\"ì¤‘ë³µ í‹°ì»¤ {len(duplicate_tickers)}ê°œ ê°€ì¤‘ì¹˜ í•©ì‚°: {duplicate_tickers}\")\n",
        "                \n",
        "                for ticker in duplicate_tickers:\n",
        "                    original_weight = original_df[original_df['Ticker'] == ticker]['Weight (%)'].sum()\n",
        "                    print(f\"    - {ticker}: ì´ ê°€ì¤‘ì¹˜ {original_weight:.2f}%ë¡œ í•©ì‚°\")\n",
        "            \n",
        "            # 3. ê°€ì¤‘ì¹˜ ì •ê·œí™” (100%ë¡œ ë§ì¶”ê¸°)\n",
        "            total_weight = df['Weight (%)'].sum()\n",
        "            if abs(total_weight - 100.0) > 0.01:\n",
        "                print(f\"  ğŸ“Š ê°€ì¤‘ì¹˜ ì •ê·œí™”: {total_weight:.2f}% â†’ 100.00%\")\n",
        "                df['Weight (%)'] = df['Weight (%)'] / total_weight * 100.0\n",
        "                fixes_for_file.append(f\"ê°€ì¤‘ì¹˜ ì •ê·œí™”: {total_weight:.2f}% â†’ 100.00%\")\n",
        "            \n",
        "            # 4. ë³€ê²½ì‚¬í•­ì´ ìˆìœ¼ë©´ íŒŒì¼ ì €ì¥\n",
        "            if fixes_for_file:\n",
        "                # ë°±ì—… íŒŒì¼ ìƒì„±\n",
        "                backup_file = file.replace('.csv', '_backup.csv')\n",
        "                shutil.copy2(file, backup_file)\n",
        "                \n",
        "                # ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥\n",
        "                df.to_csv(file, index=False)\n",
        "                \n",
        "                fixes_applied.append({\n",
        "                    'file': filename,\n",
        "                    'fixes': fixes_for_file,\n",
        "                    'backup': backup_file\n",
        "                })\n",
        "                \n",
        "                print(f\"  ğŸ’¾ ìˆ˜ì •ì‚¬í•­ ì €ì¥ ì™„ë£Œ (ë°±ì—…: {os.path.basename(backup_file)})\")\n",
        "            else:\n",
        "                print(\"  âœ… ìˆ˜ì •í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
        "    \n",
        "    # ìˆ˜ì • ìš”ì•½ ì¶œë ¥\n",
        "    print(f\"\\\\nğŸ“‹ ì´ {len(fixes_applied)}ê°œ íŒŒì¼ì—ì„œ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œë¥¼ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤:\")\n",
        "    for fix_info in fixes_applied:\n",
        "        print(f\"\\\\nğŸ“ {fix_info['file']}:\")\n",
        "        for fix in fix_info['fixes']:\n",
        "            print(f\"  - {fix}\")\n",
        "    \n",
        "    if fixes_applied:\n",
        "        print(\"\\\\nâš ï¸  ì›ë³¸ íŒŒì¼ë“¤ì€ '_backup.csv' í˜•íƒœë¡œ ë°±ì—…ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "        print(\"âœ… ë°ì´í„° í’ˆì§ˆ ìˆ˜ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ë°±í…ŒìŠ¤íŒ…ì„ ë‹¤ì‹œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "    \n",
        "    return fixes_applied\n",
        "\n",
        "# ë°ì´í„° í’ˆì§ˆ ìë™ ìˆ˜ì • ì‹¤í–‰\n",
        "fixes = fix_altman_data_quality()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìˆ˜ì • í›„ ë°ì´í„° í’ˆì§ˆ ì¬ê²€ì‚¬\n",
        "print(\"\\\\nğŸ” ë°ì´í„° í’ˆì§ˆ ìˆ˜ì • í›„ ì¬ê²€ì‚¬\")\n",
        "print(\"=\" * 50)\n",
        "issues_after = check_altman_data_quality()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê°œì„ ëœ ë°ì´í„°ë¡œ ê°„ë‹¨í•œ ë°±í…ŒìŠ¤íŒ… ë¹„êµ ì‹¤í–‰\n",
        "print(\"\\\\nğŸš€ ê°œì„ ëœ ë°ì´í„°ë¡œ Edward Altman ì—ì´ì „íŠ¸ ë°±í…ŒìŠ¤íŒ… ë¹„êµ\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def simple_altman_backtest_comparison():\n",
        "    \"\"\"ê°œì„  ì „í›„ Altman ì—ì´ì „íŠ¸ ë°ì´í„° ë¹„êµ\"\"\"\n",
        "    \n",
        "    # 1. ê°œì„  ì „ ë°ì´í„° (ë°±ì—… íŒŒì¼ë“¤) ë¶„ì„\n",
        "    print(\"ğŸ“Š ê°œì„  ì „í›„ ë°ì´í„° ë¹„êµ ë¶„ì„\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    portfolio_dir = \"results/altman_agent\"\n",
        "    portfolio_files = glob.glob(f\"{portfolio_dir}/altman_portfolio_*.csv\")\n",
        "    backup_files = glob.glob(f\"{portfolio_dir}/altman_portfolio_*_backup.csv\")\n",
        "    \n",
        "    comparison_results = []\n",
        "    \n",
        "    for file in sorted(portfolio_files):\n",
        "        if '_backup' in file:\n",
        "            continue\n",
        "            \n",
        "        filename = os.path.basename(file)\n",
        "        backup_file = file.replace('.csv', '_backup.csv')\n",
        "        \n",
        "        if os.path.exists(backup_file):\n",
        "            print(f\"\\\\nğŸ“ {filename} ë¹„êµ:\")\n",
        "            \n",
        "            # ì›ë³¸ ë°ì´í„° (ë°±ì—…)\n",
        "            df_original = pd.read_csv(backup_file)\n",
        "            # ìˆ˜ì •ëœ ë°ì´í„°\n",
        "            df_fixed = pd.read_csv(file)\n",
        "            \n",
        "            # ë¹„êµ ê²°ê³¼\n",
        "            result = {\n",
        "                'file': filename,\n",
        "                'original_tickers': len(df_original),\n",
        "                'fixed_tickers': len(df_fixed),\n",
        "                'original_weight_sum': df_original['Weight (%)'].sum(),\n",
        "                'fixed_weight_sum': df_fixed['Weight (%)'].sum(),\n",
        "                'original_duplicates': len(df_original[df_original.duplicated('Ticker', keep=False)]),\n",
        "                'fixed_duplicates': len(df_fixed[df_fixed.duplicated('Ticker', keep=False)])\n",
        "            }\n",
        "            \n",
        "            print(f\"  ğŸ“ˆ í‹°ì»¤ ìˆ˜: {result['original_tickers']} â†’ {result['fixed_tickers']}\")\n",
        "            print(f\"  ğŸ“Š ê°€ì¤‘ì¹˜ í•©ê³„: {result['original_weight_sum']:.2f}% â†’ {result['fixed_weight_sum']:.2f}%\")\n",
        "            print(f\"  ğŸ”„ ì¤‘ë³µ í‹°ì»¤: {result['original_duplicates']}ê°œ â†’ {result['fixed_duplicates']}ê°œ\")\n",
        "            \n",
        "            # ê°œì„  íš¨ê³¼ ê³„ì‚°\n",
        "            weight_improvement = abs(100.0 - result['fixed_weight_sum']) < abs(100.0 - result['original_weight_sum'])\n",
        "            duplicate_improvement = result['fixed_duplicates'] < result['original_duplicates']\n",
        "            \n",
        "            if weight_improvement or duplicate_improvement:\n",
        "                print(\"  âœ… ë°ì´í„° í’ˆì§ˆ ê°œì„ ë¨\")\n",
        "            else:\n",
        "                print(\"  â„¹ï¸  ë³€ê²½ì‚¬í•­ ì—†ìŒ\")\n",
        "                \n",
        "            comparison_results.append(result)\n",
        "        else:\n",
        "            print(f\"\\\\nğŸ“ {filename}: ë°±ì—… íŒŒì¼ ì—†ìŒ (ì›ë˜ ê¹¨ë—í•œ ë°ì´í„°)\")\n",
        "    \n",
        "    # ì „ì²´ ê°œì„  ìš”ì•½\n",
        "    if comparison_results:\n",
        "        print(f\"\\\\nğŸ“‹ ì „ì²´ ê°œì„  ìš”ì•½:\")\n",
        "        print(f\"  - ì²˜ë¦¬ëœ íŒŒì¼: {len(comparison_results)}ê°œ\")\n",
        "        \n",
        "        total_duplicate_reduction = sum(r['original_duplicates'] - r['fixed_duplicates'] for r in comparison_results)\n",
        "        files_with_weight_fix = sum(1 for r in comparison_results if abs(100.0 - r['fixed_weight_sum']) < abs(100.0 - r['original_weight_sum']))\n",
        "        \n",
        "        print(f\"  - ì¤‘ë³µ í‹°ì»¤ ì œê±°: ì´ {total_duplicate_reduction}ê°œ\")\n",
        "        print(f\"  - ê°€ì¤‘ì¹˜ ì •ê·œí™”: {files_with_weight_fix}ê°œ íŒŒì¼\")\n",
        "        \n",
        "        print(\"\\\\nâœ… Edward Altman ì—ì´ì „íŠ¸ì˜ ë°ì´í„° í’ˆì§ˆì´ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "        print(\"   ì´ì œ ë°±í…ŒìŠ¤íŒ…ì—ì„œ ë¦¬ë°¸ëŸ°ì‹± ì‹œì ì˜ ê¸‰ê²©í•œ í•˜ë½ ë¬¸ì œê°€ í•´ê²°ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.\")\n",
        "    else:\n",
        "        print(\"\\\\nğŸ“‹ ê°œì„ ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ë°ì´í„°ê°€ ì´ë¯¸ ê¹¨ë—í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "    \n",
        "    return comparison_results\n",
        "\n",
        "# ë¹„êµ ë¶„ì„ ì‹¤í–‰\n",
        "comparison_results = simple_altman_backtest_comparison()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìµœì¢… ê¶Œì¥ì‚¬í•­ ë° ë‹¤ìŒ ë‹¨ê³„\n",
        "print(\"\\\\nğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­ ë° ë‹¤ìŒ ë‹¨ê³„\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\"\"\n",
        "ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ê°œì„  ì™„ë£Œ ìš”ì•½:\n",
        "\n",
        "âœ… í•´ê²°ëœ ë¬¸ì œë“¤:\n",
        "  1. ì¤‘ë³µ í‹°ì»¤ (ì˜ˆ: TXNì´ 2ë²ˆ ë“±ì¥) â†’ ê°€ì¤‘ì¹˜ í•©ì‚°ìœ¼ë¡œ í•´ê²°\n",
        "  2. í‹°ì»¤ ì˜¤íƒ€ (ì˜ˆ: CPTR â†’ CPRT) â†’ ìë™ ìˆ˜ì •\n",
        "  3. ê°€ì¤‘ì¹˜ í•©ê³„ ë¶ˆì¼ì¹˜ (â‰  100%) â†’ ì •ê·œí™”ë¡œ í•´ê²°\n",
        "  4. ì›ë³¸ ë°ì´í„° ë³´ì¡´ â†’ ë°±ì—… íŒŒì¼ ìƒì„±\n",
        "\n",
        "ğŸš€ ë‹¤ìŒ ë‹¨ê³„:\n",
        "  1. ê¸°ì¡´ 04_multi_agent_backtesting.ipynbë¥¼ ë‹¤ì‹œ ì‹¤í–‰\n",
        "  2. Edward Altman ì—ì´ì „íŠ¸ì˜ ì„±ê³¼ ê³¡ì„  í™•ì¸\n",
        "  3. ë¦¬ë°¸ëŸ°ì‹± ì‹œì ì˜ ê¸‰ê²©í•œ í•˜ë½ ë¬¸ì œ í•´ê²° ì—¬ë¶€ ê²€ì¦\n",
        "  4. ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ê³¼ì˜ ì„±ê³¼ ë¹„êµ\n",
        "\n",
        "ğŸ’¡ ë°±í…ŒìŠ¤íŒ… ì¬ì‹¤í–‰ ë°©ë²•:\n",
        "  - 04_multi_agent_backtesting.ipynbë¥¼ ì—´ê³ \n",
        "  - backtester = MultiAgentBacktester() ë¶€í„° ë‹¤ì‹œ ì‹¤í–‰\n",
        "  - plot_multi_agent_performance()ë¡œ ê²°ê³¼ í™•ì¸\n",
        "\n",
        "âš ï¸  ì£¼ì˜ì‚¬í•­:\n",
        "  - ë°±ì—… íŒŒì¼ë“¤(_backup.csv)ì€ ì‚­ì œí•˜ì§€ ë§ˆì„¸ìš”\n",
        "  - ë¬¸ì œê°€ ìƒê¸°ë©´ ë°±ì—…ì—ì„œ ë³µì› ê°€ëŠ¥í•©ë‹ˆë‹¤\n",
        "  - ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ë„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ê¶Œì¥\n",
        "\n",
        "ğŸ” Edward Altman ì—ì´ì „íŠ¸ íŠ¹ì´ì‚¬í•­:\n",
        "  - Z-score ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤ë¡œ ë™ì¼ ê°€ì¤‘ì¹˜(3%) ì‚¬ìš©\n",
        "  - ì¤‘ë³µ í‹°ì»¤ ë¬¸ì œê°€ ê°€ì¤‘ì¹˜ ê³„ì‚°ì„ ì™œê³¡ì‹œì¼°ì„ ê°€ëŠ¥ì„±\n",
        "  - ì´ì œ ì˜¬ë°”ë¥¸ ê°€ì¤‘ì¹˜ë¡œ ë°±í…ŒìŠ¤íŒ… ê°€ëŠ¥\n",
        "\"\"\")\n",
        "\n",
        "print(\"âœ… ë°ì´í„° í’ˆì§ˆ ê°œì„  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(\"   ì´ì œ 04_multi_agent_backtesting.ipynbì—ì„œ ê°œì„ ëœ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
