{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 개선된 다중 에이전트 백테스팅 시스템 (데이터 품질 검증 포함)\n",
        "\n",
        "5개 투자 에이전트의 성과를 백테스팅하되, 데이터 품질 문제(중복 티커, 오타 등)를 자동으로 감지하고 수정합니다.\n",
        "\n",
        "## 🔧 개선 사항\n",
        "- ✅ **중복 티커 감지 및 가중치 합산**\n",
        "- ✅ **티커 오타 자동 수정** (예: CPTR → CPRT)\n",
        "- ✅ **유효하지 않은 티커 제거**\n",
        "- ✅ **가중치 정규화** (합계가 100%가 되도록)\n",
        "- ✅ **데이터 품질 리포트 제공**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 개선된 다중 에이전트 백테스팅 시스템 (1/3) - 데이터 품질 검증 클래스\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "from collections import Counter\n",
        "import difflib\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import yfinance as yf\n",
        "\n",
        "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "class ImprovedMultiAgentBacktester:\n",
        "    \"\"\"데이터 품질 검증이 포함된 5개 투자 에이전트 통합 백테스팅 클래스\"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 results_dir=\"results\",\n",
        "                 ohlcv_path=\"data/nasdaq100_ohlcv.csv\",\n",
        "                 benchmark_cache_path=\"data/benchmark_data.csv\",\n",
        "                 transaction_cost=0.0001):\n",
        "        \n",
        "        self.results_dir = results_dir\n",
        "        self.ohlcv_path = ohlcv_path\n",
        "        self.benchmark_cache_path = benchmark_cache_path\n",
        "        self.transaction_cost = transaction_cost\n",
        "        \n",
        "        # 에이전트 정보 정의\n",
        "        self.agents = {\n",
        "            'graham': {\n",
        "                'name': 'Benjamin Graham',\n",
        "                'directory': 'graham_agent',\n",
        "                'portfolio_prefix': 'graham_portfolio_',\n",
        "                'color': 'blue'\n",
        "            },\n",
        "            'buffett': {\n",
        "                'name': 'Warren Buffett',\n",
        "                'directory': 'buffett_agent',\n",
        "                'portfolio_prefix': 'buffett_portfolio_',\n",
        "                'color': 'red'\n",
        "            },\n",
        "            'greenblatt': {\n",
        "                'name': 'Joel Greenblatt',\n",
        "                'directory': 'greenblatt_agent',\n",
        "                'portfolio_prefix': 'greenblatt_portfolio_',\n",
        "                'color': 'green'\n",
        "            },\n",
        "            'piotroski': {\n",
        "                'name': 'Joseph Piotroski',\n",
        "                'directory': 'piotroski_agent',\n",
        "                'portfolio_prefix': 'piotroski_portfolio_',\n",
        "                'color': 'orange'\n",
        "            },\n",
        "            'altman': {\n",
        "                'name': 'Edward Altman',\n",
        "                'directory': 'altman_agent',\n",
        "                'portfolio_prefix': 'altman_portfolio_',\n",
        "                'color': 'purple'\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # 벤치마크 티커들 정의\n",
        "        self.benchmarks = {\n",
        "            'QQQ': {'name': 'NASDAQ 100 ETF', 'color': 'black'},\n",
        "            'SPY': {'name': 'S&P 500 ETF', 'color': 'gray'}\n",
        "        }\n",
        "        \n",
        "        # 백테스팅 결과 저장용\n",
        "        self.backtest_results = {}\n",
        "        \n",
        "        # 데이터 품질 문제 로그\n",
        "        self.data_quality_issues = []\n",
        "        \n",
        "    def validate_and_clean_ticker(self, ticker, valid_tickers=None):\n",
        "        \"\"\"티커 검증 및 정정\"\"\"\n",
        "        if valid_tickers is None:\n",
        "            # NASDAQ 100 티커 목록에서 유효한 티커 확인\n",
        "            valid_tickers = set(self.ohlcv_df['TICKERSYMBOL'].unique())\n",
        "        \n",
        "        ticker = ticker.strip().upper()\n",
        "        \n",
        "        # 정확한 매치가 있으면 그대로 반환\n",
        "        if ticker in valid_tickers:\n",
        "            return ticker\n",
        "        \n",
        "        # 유사한 티커 찾기 (오타 수정)\n",
        "        close_matches = difflib.get_close_matches(ticker, valid_tickers, n=1, cutoff=0.8)\n",
        "        if close_matches:\n",
        "            corrected_ticker = close_matches[0]\n",
        "            self.data_quality_issues.append({\n",
        "                'type': 'ticker_correction',\n",
        "                'original': ticker,\n",
        "                'corrected': corrected_ticker,\n",
        "                'message': f\"티커 '{ticker}'를 '{corrected_ticker}'로 수정했습니다.\"\n",
        "            })\n",
        "            return corrected_ticker\n",
        "        \n",
        "        # 매치되는 것이 없으면 None 반환\n",
        "        self.data_quality_issues.append({\n",
        "            'type': 'invalid_ticker',\n",
        "            'original': ticker,\n",
        "            'message': f\"유효하지 않은 티커 '{ticker}'가 발견되었습니다.\"\n",
        "        })\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 정리 및 검증 메서드들 (2/3)\n",
        "\n",
        "def clean_portfolio_data(self, df, agent_key):\n",
        "    \"\"\"포트폴리오 데이터 정리 및 검증\"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        return df\n",
        "        \n",
        "    # 원본 데이터 백업\n",
        "    original_df = df.copy()\n",
        "    \n",
        "    # 1. 빈 행 제거\n",
        "    df = df.dropna(subset=['Ticker'])\n",
        "    \n",
        "    # 2. 티커 검증 및 정정\n",
        "    valid_tickers = set(self.ohlcv_df['TICKERSYMBOL'].unique())\n",
        "    df['Original_Ticker'] = df['Ticker'].copy()\n",
        "    df['Ticker'] = df['Ticker'].apply(lambda x: self.validate_and_clean_ticker(x, valid_tickers))\n",
        "    \n",
        "    # 3. 유효하지 않은 티커 제거\n",
        "    invalid_mask = df['Ticker'].isnull()\n",
        "    if invalid_mask.any():\n",
        "        invalid_tickers = df[invalid_mask]['Original_Ticker'].tolist()\n",
        "        self.data_quality_issues.append({\n",
        "            'type': 'removed_invalid_tickers',\n",
        "            'agent': self.agents[agent_key]['name'],\n",
        "            'tickers': invalid_tickers,\n",
        "            'message': f\"{self.agents[agent_key]['name']}에서 유효하지 않은 티커 {len(invalid_tickers)}개를 제거했습니다: {invalid_tickers}\"\n",
        "        })\n",
        "        df = df[~invalid_mask]\n",
        "    \n",
        "    # 4. 중복 티커 처리\n",
        "    duplicate_tickers = df[df.duplicated('Ticker', keep=False)]['Ticker'].tolist()\n",
        "    if duplicate_tickers:\n",
        "        # 중복된 티커들의 가중치를 합산\n",
        "        self.data_quality_issues.append({\n",
        "            'type': 'duplicate_tickers',\n",
        "            'agent': self.agents[agent_key]['name'],\n",
        "            'tickers': list(set(duplicate_tickers)),\n",
        "            'message': f\"{self.agents[agent_key]['name']}에서 중복 티커 {len(set(duplicate_tickers))}개를 발견하여 가중치를 합산했습니다: {list(set(duplicate_tickers))}\"\n",
        "        })\n",
        "        \n",
        "        # 중복 티커의 가중치 합산\n",
        "        df = df.groupby('Ticker').agg({\n",
        "            'Weight (%)': 'sum',\n",
        "            'Score': 'mean',  # Score는 평균\n",
        "            'Reason': 'first'  # Reason은 첫 번째 것 사용\n",
        "        }).reset_index()\n",
        "    \n",
        "    # 5. 가중치 정규화\n",
        "    total_weight = df['Weight (%)'].sum()\n",
        "    if abs(total_weight - 100.0) > 0.01:  # 0.01% 이상 차이나면 정규화\n",
        "        self.data_quality_issues.append({\n",
        "            'type': 'weight_normalization',\n",
        "            'agent': self.agents[agent_key]['name'],\n",
        "            'original_total': total_weight,\n",
        "            'message': f\"{self.agents[agent_key]['name']}의 총 가중치가 {total_weight:.2f}%였으므로 100%로 정규화했습니다.\"\n",
        "        })\n",
        "        df['Weight (%)'] = df['Weight (%)'] / total_weight * 100.0\n",
        "    \n",
        "    # 6. 가중치가 0 이하인 항목 제거\n",
        "    zero_weight_mask = df['Weight (%)'] <= 0\n",
        "    if zero_weight_mask.any():\n",
        "        zero_weight_tickers = df[zero_weight_mask]['Ticker'].tolist()\n",
        "        self.data_quality_issues.append({\n",
        "            'type': 'removed_zero_weight',\n",
        "            'agent': self.agents[agent_key]['name'],\n",
        "            'tickers': zero_weight_tickers,\n",
        "            'message': f\"{self.agents[agent_key]['name']}에서 가중치가 0 이하인 티커 {len(zero_weight_tickers)}개를 제거했습니다: {zero_weight_tickers}\"\n",
        "        })\n",
        "        df = df[~zero_weight_mask]\n",
        "    \n",
        "    # Original_Ticker 컬럼 제거\n",
        "    if 'Original_Ticker' in df.columns:\n",
        "        df = df.drop('Original_Ticker', axis=1)\n",
        "    \n",
        "    return df\n",
        "    \n",
        "def print_data_quality_report(self):\n",
        "    \"\"\"데이터 품질 문제 리포트 출력\"\"\"\n",
        "    if not self.data_quality_issues:\n",
        "        print(\"✅ 데이터 품질 검사 완료: 문제가 발견되지 않았습니다.\")\n",
        "        return\n",
        "        \n",
        "    print(\"\\\\n\" + \"=\" * 80)\n",
        "    print(\"📋 데이터 품질 문제 리포트\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # 문제 유형별 분류\n",
        "    issue_types = Counter([issue['type'] for issue in self.data_quality_issues])\n",
        "    \n",
        "    print(f\"\\\\n총 {len(self.data_quality_issues)}개의 데이터 품질 문제를 발견하고 수정했습니다:\")\n",
        "    for issue_type, count in issue_types.items():\n",
        "        type_names = {\n",
        "            'ticker_correction': '티커 오타 수정',\n",
        "            'invalid_ticker': '유효하지 않은 티커',\n",
        "            'duplicate_tickers': '중복 티커',\n",
        "            'weight_normalization': '가중치 정규화',\n",
        "            'removed_invalid_tickers': '유효하지 않은 티커 제거',\n",
        "            'removed_zero_weight': '0 가중치 티커 제거'\n",
        "        }\n",
        "        print(f\"  - {type_names.get(issue_type, issue_type)}: {count}건\")\n",
        "    \n",
        "    print(\"\\\\n[상세 내용]\")\n",
        "    for i, issue in enumerate(self.data_quality_issues, 1):\n",
        "        print(f\"{i:2d}. {issue['message']}\")\n",
        "    \n",
        "    print(\"\\\\n\" + \"=\" * 80)\n",
        "\n",
        "# 클래스에 메서드 추가\n",
        "ImprovedMultiAgentBacktester.clean_portfolio_data = clean_portfolio_data\n",
        "ImprovedMultiAgentBacktester.print_data_quality_report = print_data_quality_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 기존 MultiAgentBacktester의 나머지 메서드들을 복사해서 사용\n",
        "# (벤치마크 데이터, 백테스팅 계산, 시각화 등)\n",
        "\n",
        "# 기존 04_multi_agent_backtesting.ipynb의 메서드들을 그대로 사용하되 \n",
        "# 데이터 로딩 부분에 품질 검증을 추가\n",
        "\n",
        "def load_data(self):\n",
        "    \"\"\"필요한 데이터들을 로드 (품질 검증 포함)\"\"\"\n",
        "    print(\"데이터 로딩 중...\")\n",
        "    \n",
        "    # OHLCV 데이터 로드\n",
        "    print(\"  - OHLCV 데이터 로딩...\")\n",
        "    self.ohlcv_df = pd.read_csv(self.ohlcv_path)\n",
        "    self.ohlcv_df['EVAL_D'] = pd.to_datetime(self.ohlcv_df['EVAL_D'])\n",
        "    \n",
        "    # 각 에이전트별 포트폴리오 파일들 로드\n",
        "    print(\"  - 에이전트별 포트폴리오 결과 파일들 로딩...\")\n",
        "    self.agent_portfolios = {}\n",
        "    for agent_key, agent_info in self.agents.items():\n",
        "        self.load_agent_portfolio_files(agent_key, agent_info)\n",
        "    \n",
        "    # 벤치마크 데이터 로드\n",
        "    print(\"  - 벤치마크 데이터 로딩...\")\n",
        "    self.benchmark_df = self.load_benchmark_data()\n",
        "    \n",
        "    print(\"데이터 로딩 완료!\")\n",
        "    \n",
        "    # 데이터 품질 리포트 출력\n",
        "    self.print_data_quality_report()\n",
        "    \n",
        "def load_agent_portfolio_files(self, agent_key, agent_info):\n",
        "    \"\"\"특정 에이전트의 포트폴리오 CSV 파일들을 로드 (데이터 품질 검증 포함)\"\"\"\n",
        "    portfolio_dir = os.path.join(self.results_dir, agent_info['directory'])\n",
        "    portfolio_files = glob.glob(f\"{portfolio_dir}/{agent_info['portfolio_prefix']}*.csv\")\n",
        "    \n",
        "    portfolios = []\n",
        "    \n",
        "    for file in sorted(portfolio_files):\n",
        "        try:\n",
        "            # 파일명에서 날짜 추출\n",
        "            filename = os.path.basename(file)\n",
        "            date_part = filename.replace(agent_info['portfolio_prefix'], '').replace('.csv', '')\n",
        "            start_date, end_date = date_part.split('_')\n",
        "            \n",
        "            # CSV 파일 로드\n",
        "            df = pd.read_csv(file)\n",
        "            if len(df) == 0 or (len(df) == 1 and df.iloc[0].isnull().all()):\n",
        "                continue\n",
        "            \n",
        "            # 🔧 데이터 품질 검증 및 정리 (이 부분이 핵심 개선사항!)\n",
        "            df = self.clean_portfolio_data(df, agent_key)\n",
        "            \n",
        "            if len(df) > 0:\n",
        "                df['analysis_start'] = pd.to_datetime(start_date)\n",
        "                df['analysis_end'] = pd.to_datetime(end_date)\n",
        "                df['investment_start'] = df['analysis_end'] + timedelta(days=1)\n",
        "                next_quarter_start = df['analysis_end'] + timedelta(days=90)\n",
        "                df['investment_end'] = next_quarter_start\n",
        "                \n",
        "                portfolios.append(df)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"파일 로딩 실패: {file}, 오류: {e}\")\n",
        "            \n",
        "    if portfolios:\n",
        "        self.agent_portfolios[agent_key] = pd.concat(portfolios, ignore_index=True)\n",
        "        print(f\"  - {agent_info['name']}: {len(portfolios)}개 분기 포트폴리오 로드됨\")\n",
        "    else:\n",
        "        print(f\"  - {agent_info['name']}: 포트폴리오 데이터 없음\")\n",
        "        self.agent_portfolios[agent_key] = None\n",
        "\n",
        "# 클래스에 메서드 추가\n",
        "ImprovedMultiAgentBacktester.load_data = load_data\n",
        "ImprovedMultiAgentBacktester.load_agent_portfolio_files = load_agent_portfolio_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 개선된 백테스터 초기화 및 데이터 품질 검사 실행\n",
        "print(\"🔧 개선된 다중 에이전트 백테스터 초기화 중...\")\n",
        "backtester = ImprovedMultiAgentBacktester(transaction_cost=0.0001)\n",
        "\n",
        "# 데이터 로딩 (자동으로 품질 검증 수행)\n",
        "backtester.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Edward Altman 에이전트의 데이터 품질 문제 직접 확인\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def check_altman_data_quality():\n",
        "    \"\"\"Altman 에이전트의 데이터 품질 문제를 확인\"\"\"\n",
        "    print(\"🔍 Edward Altman 에이전트 데이터 품질 검사\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    portfolio_dir = \"results/altman_agent\"\n",
        "    portfolio_files = glob.glob(f\"{portfolio_dir}/altman_portfolio_*.csv\")\n",
        "    \n",
        "    issues_found = []\n",
        "    \n",
        "    for file in sorted(portfolio_files):\n",
        "        filename = os.path.basename(file)\n",
        "        print(f\"\\\\n📁 {filename}\")\n",
        "        \n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            if len(df) == 0:\n",
        "                continue\n",
        "                \n",
        "            # 1. 중복 티커 검사\n",
        "            duplicates = df[df.duplicated('Ticker', keep=False)]\n",
        "            if len(duplicates) > 0:\n",
        "                duplicate_tickers = duplicates['Ticker'].unique().tolist()\n",
        "                print(f\"  ⚠️  중복 티커 발견: {duplicate_tickers}\")\n",
        "                issues_found.append(f\"{filename}: 중복 티커 {duplicate_tickers}\")\n",
        "                \n",
        "                # 중복 상세 정보 출력\n",
        "                for ticker in duplicate_tickers:\n",
        "                    ticker_rows = df[df['Ticker'] == ticker]\n",
        "                    print(f\"    - {ticker}: {len(ticker_rows)}번 등장, 가중치 합계: {ticker_rows['Weight (%)'].sum():.2f}%\")\n",
        "            \n",
        "            # 2. 가중치 합계 검사\n",
        "            total_weight = df['Weight (%)'].sum()\n",
        "            if abs(total_weight - 100.0) > 0.01:\n",
        "                print(f\"  ⚠️  가중치 합계 이상: {total_weight:.2f}% (100%가 아님)\")\n",
        "                issues_found.append(f\"{filename}: 가중치 합계 {total_weight:.2f}%\")\n",
        "            \n",
        "            # 3. 이상한 티커 검사 (예: CPTR)\n",
        "            suspicious_tickers = []\n",
        "            for ticker in df['Ticker'].unique():\n",
        "                if ticker in ['CPTR']:  # CPRT의 오타로 추정\n",
        "                    suspicious_tickers.append(ticker)\n",
        "            \n",
        "            if suspicious_tickers:\n",
        "                print(f\"  ⚠️  의심스러운 티커: {suspicious_tickers} (오타 가능성)\")\n",
        "                issues_found.append(f\"{filename}: 의심스러운 티커 {suspicious_tickers}\")\n",
        "            \n",
        "            if len(duplicates) == 0 and abs(total_weight - 100.0) <= 0.01 and len(suspicious_tickers) == 0:\n",
        "                print(\"  ✅ 문제 없음\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ 파일 읽기 오류: {e}\")\n",
        "            issues_found.append(f\"{filename}: 파일 읽기 오류\")\n",
        "    \n",
        "    print(f\"\\\\n📋 총 {len(issues_found)}개의 데이터 품질 문제 발견:\")\n",
        "    for issue in issues_found:\n",
        "        print(f\"  - {issue}\")\n",
        "    \n",
        "    return issues_found\n",
        "\n",
        "# 데이터 품질 검사 실행\n",
        "issues = check_altman_data_quality()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 품질 검사 실행\n",
        "issues = check_altman_data_quality()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 품질 문제 자동 수정 함수\n",
        "import shutil\n",
        "\n",
        "def fix_altman_data_quality():\n",
        "    \"\"\"Altman 에이전트의 데이터 품질 문제를 자동으로 수정\"\"\"\n",
        "    print(\"🔧 Edward Altman 에이전트 데이터 품질 자동 수정\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    portfolio_dir = \"results/altman_agent\"\n",
        "    portfolio_files = glob.glob(f\"{portfolio_dir}/altman_portfolio_*.csv\")\n",
        "    \n",
        "    fixes_applied = []\n",
        "    \n",
        "    for file in sorted(portfolio_files):\n",
        "        filename = os.path.basename(file)\n",
        "        print(f\"\\\\n📁 {filename} 처리 중...\")\n",
        "        \n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            if len(df) == 0:\n",
        "                continue\n",
        "                \n",
        "            original_df = df.copy()\n",
        "            fixes_for_file = []\n",
        "            \n",
        "            # 1. 티커 오타 수정 (CPTR → CPRT)\n",
        "            if 'CPTR' in df['Ticker'].values:\n",
        "                df.loc[df['Ticker'] == 'CPTR', 'Ticker'] = 'CPRT'\n",
        "                fixes_for_file.append(\"CPTR → CPRT 오타 수정\")\n",
        "                print(\"  ✅ CPTR를 CPRT로 수정했습니다.\")\n",
        "            \n",
        "            # 2. 중복 티커 처리 (가중치 합산)\n",
        "            duplicates = df[df.duplicated('Ticker', keep=False)]\n",
        "            if len(duplicates) > 0:\n",
        "                duplicate_tickers = duplicates['Ticker'].unique().tolist()\n",
        "                print(f\"  🔄 중복 티커 처리: {duplicate_tickers}\")\n",
        "                \n",
        "                # 중복 티커의 가중치 합산\n",
        "                df_fixed = df.groupby('Ticker').agg({\n",
        "                    'Weight (%)': 'sum',\n",
        "                    'Score': 'mean',\n",
        "                    'Reason': 'first'\n",
        "                }).reset_index()\n",
        "                \n",
        "                df = df_fixed\n",
        "                fixes_for_file.append(f\"중복 티커 {len(duplicate_tickers)}개 가중치 합산: {duplicate_tickers}\")\n",
        "                \n",
        "                for ticker in duplicate_tickers:\n",
        "                    original_weight = original_df[original_df['Ticker'] == ticker]['Weight (%)'].sum()\n",
        "                    print(f\"    - {ticker}: 총 가중치 {original_weight:.2f}%로 합산\")\n",
        "            \n",
        "            # 3. 가중치 정규화 (100%로 맞추기)\n",
        "            total_weight = df['Weight (%)'].sum()\n",
        "            if abs(total_weight - 100.0) > 0.01:\n",
        "                print(f\"  📊 가중치 정규화: {total_weight:.2f}% → 100.00%\")\n",
        "                df['Weight (%)'] = df['Weight (%)'] / total_weight * 100.0\n",
        "                fixes_for_file.append(f\"가중치 정규화: {total_weight:.2f}% → 100.00%\")\n",
        "            \n",
        "            # 4. 변경사항이 있으면 파일 저장\n",
        "            if fixes_for_file:\n",
        "                # 백업 파일 생성\n",
        "                backup_file = file.replace('.csv', '_backup.csv')\n",
        "                shutil.copy2(file, backup_file)\n",
        "                \n",
        "                # 수정된 파일 저장\n",
        "                df.to_csv(file, index=False)\n",
        "                \n",
        "                fixes_applied.append({\n",
        "                    'file': filename,\n",
        "                    'fixes': fixes_for_file,\n",
        "                    'backup': backup_file\n",
        "                })\n",
        "                \n",
        "                print(f\"  💾 수정사항 저장 완료 (백업: {os.path.basename(backup_file)})\")\n",
        "            else:\n",
        "                print(\"  ✅ 수정할 내용이 없습니다.\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ 파일 처리 오류: {e}\")\n",
        "    \n",
        "    # 수정 요약 출력\n",
        "    print(f\"\\\\n📋 총 {len(fixes_applied)}개 파일에서 데이터 품질 문제를 수정했습니다:\")\n",
        "    for fix_info in fixes_applied:\n",
        "        print(f\"\\\\n📁 {fix_info['file']}:\")\n",
        "        for fix in fix_info['fixes']:\n",
        "            print(f\"  - {fix}\")\n",
        "    \n",
        "    if fixes_applied:\n",
        "        print(\"\\\\n⚠️  원본 파일들은 '_backup.csv' 형태로 백업되었습니다.\")\n",
        "        print(\"✅ 데이터 품질 수정이 완료되었습니다. 이제 백테스팅을 다시 실행할 수 있습니다.\")\n",
        "    \n",
        "    return fixes_applied\n",
        "\n",
        "# 데이터 품질 자동 수정 실행\n",
        "fixes = fix_altman_data_quality()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 수정 후 데이터 품질 재검사\n",
        "print(\"\\\\n🔍 데이터 품질 수정 후 재검사\")\n",
        "print(\"=\" * 50)\n",
        "issues_after = check_altman_data_quality()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 개선된 데이터로 간단한 백테스팅 비교 실행\n",
        "print(\"\\\\n🚀 개선된 데이터로 Edward Altman 에이전트 백테스팅 비교\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def simple_altman_backtest_comparison():\n",
        "    \"\"\"개선 전후 Altman 에이전트 데이터 비교\"\"\"\n",
        "    \n",
        "    # 1. 개선 전 데이터 (백업 파일들) 분석\n",
        "    print(\"📊 개선 전후 데이터 비교 분석\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    portfolio_dir = \"results/altman_agent\"\n",
        "    portfolio_files = glob.glob(f\"{portfolio_dir}/altman_portfolio_*.csv\")\n",
        "    backup_files = glob.glob(f\"{portfolio_dir}/altman_portfolio_*_backup.csv\")\n",
        "    \n",
        "    comparison_results = []\n",
        "    \n",
        "    for file in sorted(portfolio_files):\n",
        "        if '_backup' in file:\n",
        "            continue\n",
        "            \n",
        "        filename = os.path.basename(file)\n",
        "        backup_file = file.replace('.csv', '_backup.csv')\n",
        "        \n",
        "        if os.path.exists(backup_file):\n",
        "            print(f\"\\\\n📁 {filename} 비교:\")\n",
        "            \n",
        "            # 원본 데이터 (백업)\n",
        "            df_original = pd.read_csv(backup_file)\n",
        "            # 수정된 데이터\n",
        "            df_fixed = pd.read_csv(file)\n",
        "            \n",
        "            # 비교 결과\n",
        "            result = {\n",
        "                'file': filename,\n",
        "                'original_tickers': len(df_original),\n",
        "                'fixed_tickers': len(df_fixed),\n",
        "                'original_weight_sum': df_original['Weight (%)'].sum(),\n",
        "                'fixed_weight_sum': df_fixed['Weight (%)'].sum(),\n",
        "                'original_duplicates': len(df_original[df_original.duplicated('Ticker', keep=False)]),\n",
        "                'fixed_duplicates': len(df_fixed[df_fixed.duplicated('Ticker', keep=False)])\n",
        "            }\n",
        "            \n",
        "            print(f\"  📈 티커 수: {result['original_tickers']} → {result['fixed_tickers']}\")\n",
        "            print(f\"  📊 가중치 합계: {result['original_weight_sum']:.2f}% → {result['fixed_weight_sum']:.2f}%\")\n",
        "            print(f\"  🔄 중복 티커: {result['original_duplicates']}개 → {result['fixed_duplicates']}개\")\n",
        "            \n",
        "            # 개선 효과 계산\n",
        "            weight_improvement = abs(100.0 - result['fixed_weight_sum']) < abs(100.0 - result['original_weight_sum'])\n",
        "            duplicate_improvement = result['fixed_duplicates'] < result['original_duplicates']\n",
        "            \n",
        "            if weight_improvement or duplicate_improvement:\n",
        "                print(\"  ✅ 데이터 품질 개선됨\")\n",
        "            else:\n",
        "                print(\"  ℹ️  변경사항 없음\")\n",
        "                \n",
        "            comparison_results.append(result)\n",
        "        else:\n",
        "            print(f\"\\\\n📁 {filename}: 백업 파일 없음 (원래 깨끗한 데이터)\")\n",
        "    \n",
        "    # 전체 개선 요약\n",
        "    if comparison_results:\n",
        "        print(f\"\\\\n📋 전체 개선 요약:\")\n",
        "        print(f\"  - 처리된 파일: {len(comparison_results)}개\")\n",
        "        \n",
        "        total_duplicate_reduction = sum(r['original_duplicates'] - r['fixed_duplicates'] for r in comparison_results)\n",
        "        files_with_weight_fix = sum(1 for r in comparison_results if abs(100.0 - r['fixed_weight_sum']) < abs(100.0 - r['original_weight_sum']))\n",
        "        \n",
        "        print(f\"  - 중복 티커 제거: 총 {total_duplicate_reduction}개\")\n",
        "        print(f\"  - 가중치 정규화: {files_with_weight_fix}개 파일\")\n",
        "        \n",
        "        print(\"\\\\n✅ Edward Altman 에이전트의 데이터 품질이 개선되었습니다!\")\n",
        "        print(\"   이제 백테스팅에서 리밸런싱 시점의 급격한 하락 문제가 해결될 것으로 예상됩니다.\")\n",
        "    else:\n",
        "        print(\"\\\\n📋 개선된 파일이 없습니다. 원본 데이터가 이미 깨끗했을 수 있습니다.\")\n",
        "    \n",
        "    return comparison_results\n",
        "\n",
        "# 비교 분석 실행\n",
        "comparison_results = simple_altman_backtest_comparison()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 최종 권장사항 및 다음 단계\n",
        "print(\"\\\\n🎯 최종 권장사항 및 다음 단계\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\"\"\n",
        "📋 데이터 품질 개선 완료 요약:\n",
        "\n",
        "✅ 해결된 문제들:\n",
        "  1. 중복 티커 (예: TXN이 2번 등장) → 가중치 합산으로 해결\n",
        "  2. 티커 오타 (예: CPTR → CPRT) → 자동 수정\n",
        "  3. 가중치 합계 불일치 (≠ 100%) → 정규화로 해결\n",
        "  4. 원본 데이터 보존 → 백업 파일 생성\n",
        "\n",
        "🚀 다음 단계:\n",
        "  1. 기존 04_multi_agent_backtesting.ipynb를 다시 실행\n",
        "  2. Edward Altman 에이전트의 성과 곡선 확인\n",
        "  3. 리밸런싱 시점의 급격한 하락 문제 해결 여부 검증\n",
        "  4. 다른 에이전트들과의 성과 비교\n",
        "\n",
        "💡 백테스팅 재실행 방법:\n",
        "  - 04_multi_agent_backtesting.ipynb를 열고\n",
        "  - backtester = MultiAgentBacktester() 부터 다시 실행\n",
        "  - plot_multi_agent_performance()로 결과 확인\n",
        "\n",
        "⚠️  주의사항:\n",
        "  - 백업 파일들(_backup.csv)은 삭제하지 마세요\n",
        "  - 문제가 생기면 백업에서 복원 가능합니다\n",
        "  - 다른 에이전트들도 같은 방식으로 데이터 품질 검사 권장\n",
        "\n",
        "🔍 Edward Altman 에이전트 특이사항:\n",
        "  - Z-score 기반 포트폴리오로 동일 가중치(3%) 사용\n",
        "  - 중복 티커 문제가 가중치 계산을 왜곡시켰을 가능성\n",
        "  - 이제 올바른 가중치로 백테스팅 가능\n",
        "\"\"\")\n",
        "\n",
        "print(\"✅ 데이터 품질 개선 작업이 완료되었습니다!\")\n",
        "print(\"   이제 04_multi_agent_backtesting.ipynb에서 개선된 결과를 확인해보세요.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
